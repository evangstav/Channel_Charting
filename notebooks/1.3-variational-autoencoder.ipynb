{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/evs/pytorch-ignite-vae\" target=\"_blank\">https://app.wandb.ai/evs/pytorch-ignite-vae</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/evs/pytorch-ignite-vae/runs/she3vpop\" target=\"_blank\">https://app.wandb.ai/evs/pytorch-ignite-vae/runs/she3vpop</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/evs/pytorch-ignite-vae/runs/she3vpop"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"pytorch-ignite-vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.dark_palette(\"purple\"))\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import MeanSquaredError, Loss, RunningAverage\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-encoders 101\n",
    "\n",
    "In this exercise we will implement a variational auto-encoder (VAE). An auto-encoder encodes some input into a new and usually more compact representation which can be used to reconstruct the input data again. A VAE makes the assumption that the compact representation follows a probabilistic distribution (usually Gaussian) which makes it possible to sample new points and decode them into new data from a trained variational auto-encoder. The \"variational\" part comes from the fact that these models are training through variational inference.\n",
    "\n",
    "The mathematical details of the training can be a bit challenging. However, we believe that probabilistic deep learning will be an important part of future machine learning, which is why we find it important to introduce the concepts.\n",
    "\n",
    "As background material we recommend reading [Tutorial on Variational Autoencoder](http://arxiv.org/abs/1606.05908). For the implementation of the model you can read the article \"Auto-Encoding Variational Bayes\", Kingma & Welling, ICLR 2014: http://arxiv.org/pdf/1312.6114v10.pdf and \"Stochastic Backpropagation and Approximate Inference in Deep Generative Models\", Rezende et al, ICML 2014: http://arxiv.org/pdf/1401.4082v3.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE crash course\n",
    "\n",
    "Like the simple auto-encoder, VAEs consist of two parts as seen in the figure below where all arrows are non-linear mappings through a neural network. The two parts are the:\n",
    "\n",
    " * **Encoder** (also known as recognition, inference or Q-model): Maps the input data into a probabilistic latent space, z, by defining the mean and variance parameters of a Gaussian distribution as non-linear functions of the input data x like:\n",
    "     - $q(z|x) = \\mathcal{N}(z|\\mu_\\theta(x), \\sigma_\\phi(x))$, which is called the approximate posterior or latent distribution. The parameters $\\mu_\\theta(x)$ (mean) and $\\log \\sigma_\\phi(x)^2$ (log-variance) are outputs from a hidden layer each.\n",
    " * **Decoder** (also known as generative, reconstruction or P-model): Conditioned on samples drawn from $z \\sim q(z|x)$ in the encoder the input data is reconstructed through the: \n",
    "     - $p(x|z)$, which is the conditional likelihood (generative distribution). The choice of the generative distribution depends on the nature of the features, so for binary pixel values an appropiate choice of reconstruction distribution is the [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution), $p(x|z) = Ber(\\mu_\\phi(z)) = \\mu_\\phi(z)^x(1-\\mu_\\phi(z))^{1-k}$. $\\mu_\\phi(z)$ with $x=\\{0,1\\}$ is again the non-linear output of the last layer in the decoder. $\\mu_\\phi(z)$ is the probabilities of generating a 0 (black) or 1 (white) pixel value, like modelling 784 imbalanced coin-tossing processes. This is only possible because we assume the pixel intensities to be i.i.d. (Independent and Identically Distributed), so no direct correlations between them needs to modelled, even though we still achieve an indirect conditional correlation through the latent variables, z.\n",
    "     \n",
    "<img src=\"../static_files/VAE.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In more mathematical details we can get the marginal likelihood for the features, e.g. binary pixel values, by integrating out the latent variable in the joint distribution:\n",
    "\n",
    "$p(x) = \\int_z p(x,z) dz = \\int_z p(x|z)p(z)dz$\n",
    "\n",
    "As a trick to introduce the approximate posterior, $q(z|x)$, which is more feasible to compute compared to our unknown true posterior, $p(z|x)$, we can always multiply and divide by $q(z|x)$ and move them around without changing anything:\n",
    "\n",
    "$p(x) = \\int_z p(x|z)p(z)\\frac{q(z|x)}{q(z|x)}dz$\n",
    "\n",
    "\n",
    "$p(x) = \\int_z q(z|x) \\frac{p(x|z)p(z)}{q(z|x)}dz$\n",
    "\n",
    "Joint distributions can lead to underflow errors on a compute, so we instead try to maximize the log-likelihood  \n",
    "\n",
    "$\\log p(x) = \\log \\int_z q(z|x) \\frac{p(x|z)p(z)}{q(z|x)}dz = \\log \\mathbb{E}_{q(z|x)}\\left[\\frac{p(x|z)p(z)}{q(z|x)}\\right]$\n",
    "\n",
    "where we used that the integral is just the expectation (mean) wrt. $q(z|x)$ and in this case the $\\log$ can be moved inside the expectation by applying [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality):\n",
    "\n",
    "$\\log p(x) \\geq  \\int_z q(z|x)\\log \\frac{p(x|z)p(z)}{q(z|x)}dz =  \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{p(x|z)p(z)}{q(z|x)}\\right] = \\mathcal{L}(x)$\n",
    "\n",
    "This, $\\mathcal{L}(x)$, is denoted the variational lower bound or evidence lower bound objective (ELBO). It is a lower bound to the log-likelihood and a tradeoff with the Kullback-Leibler divergence, $KL[q(z|x) || p(z|x)]$, between the approximate and true posterior, which we threw away when applying Jensen's inequality. This tradeoff is more easily understood through the derivation in the end of the notebook, so read it if you have the time. \n",
    "\n",
    "Like with Pokemons, there is a whole range of families of distributions to choose from, but we choose the most common one describing symmetric variations around a mean in signals with noise, the normal distribution:\n",
    "\n",
    "$q(z|x) = \\mathcal{N}(z|\\mu_\\theta(x), \\sigma_\\phi(x)I)$\n",
    "\n",
    "and a simple isotropic normal distribution as the latent prior\n",
    "\n",
    "$p(z) = \\mathcal{N}(z|0, I)$\n",
    "\n",
    "which becomes the part of the important regularising KL-term, when splitting up the lower bound as\n",
    "\n",
    "$\\mathcal{L}(x) = \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z)\\right] - \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{q(z|x)}{p(z)}\\right] = \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z)\\right] - KL[q(z|x) || p(z)]$\n",
    "\n",
    "This is the function that we need to maximise, by minimising the negative lower bound. Here the first term on the R.H.S. is the data reconstruction and the second term the Kullback-Leibler divergence between the approximate and true posterior distributions which acts as a probabilistic regularizer forcing $q(z|x)$ to be close to having zero mean and identity variance, like $p(z)$. The KL-term can calculated analytically and the reconstruction error, $\\log p(x|z)$, is just the binary cross-entropy.\n",
    "\n",
    "### Training a VAE \n",
    "The VAE is similar to a deterministic autoencoder (1.Auto_Encoders) except that we assume that the latent units follows a distribution. Usually we just assume that the units are independent standard normally distributed (i.i.d.).\n",
    "\n",
    "Above we defined a lower bound on the log-likelihood of the data. We can train the model by maximising the lower bound w.r.t. the model parameters, weight matrices, through the stochastic gradient descent algorithm.  Feasible approximations of the expectations in the lower bound, $\\mathcal{L}(x)$, are obtained by evaluating the inside with samples drawn from the latent distribution, $z \\sim q(z|x) = \\mathcal{N}(z|\\mu_\\theta(x), \\sigma_\\phi(x)I)$ and dividing by the number of samples drawn. By using the _reparameterization trick_, $ \\mu_\\theta(x) + \\sigma_\\phi(x) \\cdot \\epsilon$, for the sampling procedure we can directly backpropogate gradients through the latent bottleneck and optimize the parameters w.r.t. the lower bound. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "First let us load the MNIST dataset and plot a few examples. We only load a limited amount of classes, controlled through the `classes` variable, to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data) :  60000\n",
      "len(val_data) :  10000\n",
      "image.shape :  torch.Size([1, 28, 28])\n",
      "label :  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_transform = Compose([ToTensor()])\n",
    "\n",
    "train_data = MNIST(download=True, root=\"/tmp/mnist/\", transform=data_transform, train=True)\n",
    "val_data = MNIST(download=True, root=\"/tmp/mnist/\", transform=data_transform, train=False)\n",
    "\n",
    "image = train_data[0][0]\n",
    "label = train_data[0][1]\n",
    "\n",
    "print ('len(train_data) : ', len(train_data))\n",
    "print ('len(val_data) : ', len(val_data))\n",
    "print ('image.shape : ', image.shape)\n",
    "print ('label : ', label)\n",
    "\n",
    "img = plt.imshow(image.squeeze().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "When defining the model the latent layer must act as a bottleneck of information, so that we ensure that we find a strong internal representation. We initialize the VAE with 1 hidden layer in the encoder and decoder using relu units as non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape :  torch.Size([32, 1, 28, 28])\n",
      "y.shape :  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, **kwargs)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=True, **kwargs)\n",
    "\n",
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    break\n",
    "\n",
    "print ('x.shape : ', x.shape)\n",
    "print ('y.shape : ', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.functional import softplus\n",
    "\n",
    "# define size variables\n",
    "num_features = 28**2\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_features, num_samples):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.latent_features = latent_features\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # We encode the data onto the latent space using two linear layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=num_features, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            # A Gaussian is fully characterised by its mean and variance\n",
    "            nn.Linear(in_features=64, out_features=2*self.latent_features) # <- note the 2*latent_features\n",
    "        )\n",
    "        \n",
    "        # The latent code must be decoded into the original image\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=self.latent_features, out_features=64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=256, out_features=num_features)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x): \n",
    "        outputs = {}\n",
    "        \n",
    "        # Split encoder outputs into a mean and variance vector\n",
    "        mu, log_var = torch.chunk(self.encoder(x), 2, dim=-1)\n",
    "        \n",
    "        # :- Reparametrisation trick\n",
    "        # a sample from N(mu, sigma) is mu + sigma * epsilon\n",
    "        # where epsilon ~ N(0, 1)\n",
    "                \n",
    "        # Don't propagate gradients through randomness\n",
    "        with torch.no_grad():\n",
    "            batch_size = mu.size(0)\n",
    "            epsilon = torch.randn(batch_size, self.num_samples, self.latent_features)\n",
    "            \n",
    "            if cuda:\n",
    "                epsilon = epsilon.cuda()\n",
    "        \n",
    "        sigma = torch.exp(log_var/2)\n",
    "        \n",
    "        # We will need to unsqueeze to turn\n",
    "        # (batch_size, latent_dim) -> (batch_size, 1, latent_dim)\n",
    "        z = mu.unsqueeze(1) + epsilon * sigma.unsqueeze(1)        \n",
    "        \n",
    "        # Run through decoder\n",
    "        x = self.decoder(z)\n",
    "        \n",
    "        # The original digits are on the scale [0, 1]\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        # Mean over samples\n",
    "        x_hat = torch.mean(x, dim=1)\n",
    "        \n",
    "        outputs[\"x_hat\"] = x_hat\n",
    "        outputs[\"z\"] = z\n",
    "        outputs[\"mu\"] = mu\n",
    "        outputs[\"log_var\"] = log_var\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "latent_features = 10\n",
    "num_samples = 10\n",
    "\n",
    "net = VariationalAutoencoder(latent_features, num_samples)\n",
    "\n",
    "# Transfer model to GPU if available\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we define the PyTorch functions for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def kld_loss(x_pred, x, mu, logvar):\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "bce_loss = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the forward pass, to check that everything is in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, _ = batch\n",
    "    x = x.to(device)\n",
    "    x = x.view(-1, 784)\n",
    "    x_pred, mu, logvar = model(x)\n",
    "    BCE = bce_loss(x_pred, x)\n",
    "    KLD = kld_loss(x_pred, x, mu, logvar)\n",
    "    loss = BCE + KLD\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), BCE.item(), KLD.item()\n",
    "\n",
    "\n",
    "def evaluate_function(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, _ = batch\n",
    "        x = x.to(device)\n",
    "        x = x.view(-1, 784)\n",
    "        x_pred, mu, logvar = model(x)\n",
    "        kwargs = {'mu': mu, 'logvar': logvar}\n",
    "        return x_pred, x, kwargs\n",
    "\n",
    "\n",
    "trainer = Engine(process_function)\n",
    "evaluator = Engine(evaluate_function)\n",
    "training_history = {'bce': [], 'kld': [], 'mse': []}\n",
    "validation_history = {'bce': [], 'kld': [], 'mse': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningAverage(output_transform=lambda x: x[0]).attach(trainer, 'loss')\n",
    "RunningAverage(output_transform=lambda x: x[1]).attach(trainer, 'bce')\n",
    "RunningAverage(output_transform=lambda x: x[2]).attach(trainer, 'kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeanSquaredError(output_transform=lambda x: [x[0], x[1]]).attach(evaluator, 'mse')\n",
    "Loss(bce_loss, output_transform=lambda x: [x[0], x[1]]).attach(evaluator, 'bce')\n",
    "Loss(kld_loss).attach(evaluator, 'kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeanSquaredError(output_transform=lambda x: [x[0], x[1]]).attach(evaluator, 'mse')\n",
    "Loss(bce_loss, output_transform=lambda x: [x[0], x[1]]).attach(evaluator, 'bce')\n",
    "Loss(kld_loss).attach(evaluator, 'kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.RemovableEventHandle at 0x7f8c4136bbb0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_logs(engine, dataloader, mode, history_dict):\n",
    "    evaluator.run(dataloader, max_epochs=1)\n",
    "    metrics = evaluator.state.metrics\n",
    "    avg_mse = metrics['mse']\n",
    "    avg_bce = metrics['bce']\n",
    "    avg_kld = metrics['kld']\n",
    "    avg_loss =  avg_bce + avg_kld\n",
    "    print(\n",
    "        mode + \" Results - Epoch {} - Avg mse: {:.2f} Avg loss: {:.2f} Avg bce: {:.2f} Avg kld: {:.2f}\"\n",
    "        .format(engine.state.epoch, avg_mse, avg_loss, avg_bce, avg_kld))\n",
    "    for key in evaluator.state.metrics.keys():\n",
    "        history_dict[key].append(evaluator.state.metrics[key])\n",
    "\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, print_logs, train_loader, 'Training', training_history)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, print_logs, val_loader, 'Validation', validation_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using more samples gives us a better approximation of the expectation, and consequently a better approximation of the lower bound. The reconstructed image is generated by taking the mean over the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional exercises\n",
    "\n",
    "- OPT: Use the original paper http://arxiv.org/pdf/1312.6114v10.pdf or [this blog](http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/) to explain what the reparameterization trick does.\n",
    "- OPT: Look through https://arxiv.org/abs/1611.00712 or https://arxiv.org/abs/1611.01144 and explain how one could instead introduce a categorical latent variable for $z$.\n",
    "- OPT: Implement the Gumbel softmax trick thereby letting $z$ take a categorical distribution.\n",
    "- OPT: The VAE is a probablistic model. We could model $p(x,z,y)$ where $y$ is the label information. Explain how this model could handle semi-supervised learning? You can look through the papers https://arxiv.org/pdf/1406.5298.pdf or  https://arxiv.org/pdf/1602.05473v4.pdf or again the two papers on Gumbel softmax.\n",
    "\n",
    "**Answers**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
